{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CbRjQLpfok-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install -U albumentations"
      ],
      "metadata": {
        "id": "LulTNnmUT3Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r \"./pova_train\"\n",
        "!mkdir -p \"./pova_train\"\n",
        "!find \"/content/drive/My Drive/pova_train\" -type f | head -n 1000 | xargs -I {} cp {} \"./pova_train\" #How many pictures to move from drive to local train folder (too many pictures will take long)"
      ],
      "metadata": {
        "id": "WbaXgDv0o2Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r \"./pova_test\"\n",
        "!mkdir -p \"./pova_test\"\n",
        "!find \"/content/drive/My Drive/pova_train\" -type f | tail -n 200 | xargs -I {} cp {} \"./pova_test\" #Same as above, just for testing"
      ],
      "metadata": {
        "id": "sJs7Wya_rfE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from google.colab.patches import cv2_imshow as show"
      ],
      "metadata": {
        "id": "IFK1Gzzil-ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Augmentation():\n",
        "  def __init__(self,\n",
        "               blur: bool = False, colorJitter: bool = False,\n",
        "               grayscale: bool = False, img_size: int = 256,\n",
        "               iterations: int = 1, threshold: float = 0.0):\n",
        "    self.img_size = img_size\n",
        "    self.iterations = iterations\n",
        "    self.threshold = threshold\n",
        "    self.augmentation_list = [\n",
        "        A.RandomCrop(width=self.img_size, height=self.img_size),\n",
        "        A.Rotate(limit=[-360,360]),\n",
        "        A.RandomToneCurve(scale=0.4),\n",
        "    ]\n",
        "    if blur:\n",
        "      self.augmentation_list.append(A.Blur(blur_limit=(5,7)))\n",
        "    if colorJitter:\n",
        "      self.augmentation_list.append(A.ColorJitter())\n",
        "    if grayscale:\n",
        "      self.augmentation_list.append(A.ToGray())\n",
        "\n",
        "  @staticmethod\n",
        "  def road_coverage(mask: np.ndarray) -> float:\n",
        "    return np.sum(mask // 255) / mask.size\n",
        "\n",
        "  def __call__(self, image: np.ndarray, mask: np.ndarray) -> dict[str, np.ndarray]:\n",
        "    transform = A.Compose(self.augmentation_list)\n",
        "    for i in range(self.iterations):\n",
        "      result = transform(image=image, mask=mask)\n",
        "      if self.road_coverage(result[\"mask\"]) > self.threshold:\n",
        "        break\n",
        "    return result"
      ],
      "metadata": {
        "id": "9noFFhTL-CIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataset_dir, transform=None, clip = 0):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        files = os.listdir(dataset_dir)\n",
        "        images = {f.split('_')[0]: os.path.join(dataset_dir, f) for f in files if f.endswith('_sat.jpg')}\n",
        "        labels = {f.split('_')[0]: os.path.join(dataset_dir, f) for f in files if f.endswith('_mask.png')}\n",
        "        self.image_data = [(images[key], labels[key]) for key in images if key in labels]\n",
        "        if(clip > 0):\n",
        "            self.image_data = self.image_data[:clip]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label_path = self.image_data[idx]\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "          return self.transform(image, label)\n",
        "\n",
        "        # consistent output format regardless of transform\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"mask\": label,\n",
        "        }\n",
        "        \"\"\"\n",
        "        TODO - probably remove ? did not know if it is needed somewhere\n",
        "        image = image / 255.0\n",
        "        label = label / 255.0\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(torch.tensor(image, dtype=torch.float32).permute(2, 0, 1))\n",
        "        else:\n",
        "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "        if self.label_transform:\n",
        "            label = self.label_transform(torch.tensor(label, dtype=torch.float32).unsqueeze(0))\n",
        "        else:\n",
        "            label = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        return image, label\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "y72Kjd7HlsAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST DATA AUGMENTATION\n",
        "dataset = CustomImageDataset(\"./pova_train\", transform=Augmentation(threshold=0.025, iterations=10, colorJitter=True),clip=1000)\n",
        "for i in range(5):\n",
        "  data = dataset[0]\n",
        "  show(data[\"image\"])\n",
        "  show(data[\"mask\"])\n",
        "  print(Augmentation.road_coverage(data[\"mask\"]))"
      ],
      "metadata": {
        "id": "_APVKAp4-YWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dice loss function\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, preds, targets, smooth=1):\n",
        "        preds = torch.sigmoid(preds)  # Apply sigmoid for probabilities\n",
        "        preds = preds.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (preds * targets).sum()\n",
        "        dice = 1 - ((2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth))\n",
        "        return dice"
      ],
      "metadata": {
        "id": "nLAZwzmWDQzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment params\n",
        "num_epochs = 20\n",
        "\n",
        "#Initialize dataset\n",
        "dataset = CustomImageDataset(\"./pova_train\", clip=1000)\n",
        "#Split into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "#Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "#Define model\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=\"efficientnet-b4\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=1\n",
        ")\n",
        "\n",
        "#Define loss and optimizer\n",
        "loss_fn = DiceLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "PtJalH6leKnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLd5JC7y-OFr"
      },
      "outputs": [],
      "source": [
        "#Move model to device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_loss_record = []\n",
        "val_loss_record = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        #Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        #Compute loss\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        #Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "    #Keep track of train loss scores\n",
        "    train_loss_record.append(train_loss)\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    #Keep track of validation loss scores\n",
        "    val_loss_record.append(val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_record, label='Training Loss')\n",
        "plt.plot(val_loss_record, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CzhSXlIrB7lN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NOTE: This is here just to visually check the model\n",
        "\n",
        "#Load the test dataset\n",
        "test_dataset = CustomImageDataset(\"./pova_test\", transform=None)\n",
        "\n",
        "#Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "#Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "#Function to display an image, predicted mask, and actual mask\n",
        "def display_prediction(image, predicted_mask, actual_mask):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    #Input image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())  # Convert CHW to HWC for display\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    #Model prediction\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(predicted_mask.cpu().numpy(), cmap='gray')\n",
        "    plt.title(\"Model Prediction\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    #Ground truth\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(actual_mask.squeeze(0).cpu().numpy(), cmap='gray')\n",
        "    plt.title(\"Actual Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#Iterate through the test dataset\n",
        "for idx in range(len(test_dataset)):\n",
        "    #Get the image and label\n",
        "    image, actual_mask = test_dataset[idx]\n",
        "    image = image.to(device).unsqueeze(0)  #Add batch dimension\n",
        "    actual_mask = actual_mask.to(device)\n",
        "\n",
        "    #Run the image through the model\n",
        "    with torch.no_grad():\n",
        "        predicted_mask = model(image)\n",
        "        predicted_mask = torch.sigmoid(predicted_mask)  #Apply sigmoid for binary segmentation\n",
        "        predicted_mask = (predicted_mask > 0.5).float().squeeze(0)  #Threshold to binary mask\n",
        "\n",
        "    #Display the input image, model prediction, and actual mask\n",
        "    display_prediction(image.squeeze(0), predicted_mask.squeeze(0), actual_mask)"
      ],
      "metadata": {
        "id": "B-MLbaQEAAO5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}